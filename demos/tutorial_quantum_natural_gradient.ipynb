{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\nQuantum natural gradient\n========================\n\nThis example demonstrates the quantum natural gradient optimization technique\nfor variational quantum circuits, originally proposed in\n`Stokes et al. (2019) <https://arxiv.org/abs/1909.02108>`__.\n\nBackground\n----------\n\nThe most successful class of quantum algorithms for use on near-term noisy quantum hardware\nis the so-called variational quantum algorithm. As laid out in the `Concepts section <varcirc>`, in variational quantum algorithms\na low-depth parametrized quantum circuit ansatz is chosen, and a problem-specific\nobservable measured. A classical optimization loop is then used to find\nthe set of quantum parameters that *minimize* a particular measurement expectation value\nof the quantum device. Examples of such algorithms include the `variational quantum\neigensolver (VQE) <vqe>`, the `quantum approximate optimization algorithm (QAOA) <https://arxiv.org/abs/1411.4028>`__,\nand `quantum neural networks (QNN) <quantum_neural_net>`.\n\nMost recent demonstrations\nof variational quantum algorithms have used gradient-free classical optimization\nmethods, such as the Nelder-Mead algorithm. However, the parameter-shift rule\n(as implemented in PennyLane) allows the user to automatically compute\nanalytic gradients of quantum circuits. This opens up the possibility to train\nquantum computing hardware using gradient descent---the same method used to train\ndeep learning models.\nThough one caveat has surfaced with gradient descent --- how do we choose the optimal\nstep size for our variational quantum algorithms, to ensure successful and\nefficient optimization?\n\nThe natural gradient\n^^^^^^^^^^^^^^^^^^^^\n\nIn standard gradient descent, each optimization step is given by\n\n\\begin{align}\\theta_{t+1} = \\theta_t -\\eta \\nabla \\mathcal{L}(\\theta),\\end{align}\n\nwhere $\\mathcal{L}(\\theta)$ is the cost as a function of\nthe parameters $\\theta$, and $\\eta$ is the learning rate\nor step size. In essence, each optimization step calculates the\nsteepest descent direction around the local value of $\\theta_t$\nin the parameter space, and updates $\\theta_t\\rightarrow \\theta_{t+1}$\nby this vector.\n\nThe problem with the above approach is that each optimization step\nis strongly connected to a *Euclidean geometry* on the parameter space.\nThe parametrization is not unique, and different parametrizations can distort\ndistances within the optimization landscape.\n\nFor example, consider the following cost function $\\mathcal{L}$, parametrized\nusing two different coordinate systems, $(\\theta_0, \\theta_1)$, and\n$(\\phi_0, \\phi_1)$:\n\n|\n\n.. figure:: ../demonstrations/quantum_natural_gradient/qng7.png\n    :align: center\n    :width: 90%\n    :target: javascript:void(0)\n\n|\n\nPerforming gradient descent in the $(\\theta_0, \\theta_1)$ parameter\nspace, we are updating each parameter by the same Euclidean distance,\nand not taking into account the fact that the cost function might vary at a different\nrate with respect to each parameter.\n\nInstead, if we perform a change of coordinate system (re-parametrization)\nof the cost function, we might find a parameter space where variations in $\\mathcal{L}$\nare similar across different parameters. This is the case with the new parametrization\n$(\\phi_0, \\phi_1)$; the cost function is unchanged,\nbut we now have a nicer geometry in which to perform gradient descent, and a more\ninformative stepsize. This leads to faster convergence, and can help avoid optimization\nbecoming stuck in local minima. For a more in-depth explanation,\nincluding why the parameter space might not be best represented by a Euclidean space,\nsee `Yamamoto (2019) <https://arxiv.org/abs/1909.05074>`__.\n\nHowever, what if we avoid gradient descent in the parameter space altogether?\nIf we instead consider the optimization problem as a\nprobability distribution of possible output values given an input\n(i.e., `maximum likelihood estimation <https://en.wikipedia.org/wiki/Likelihood_function>`_),\na better approach is to perform the gradient descent in the *distribution space*, which is\ndimensionless and invariant with respect to the parametrization. As a result,\neach optimization step will always choose the optimum step-size for every\nparameter, regardless of the parametrization.\n\nIn classical neural networks, the above process is known as\n*natural gradient descent*, and was first introduced by\n`Amari (1998) <https://www.mitpressjournals.org/doi/abs/10.1162/089976698300017746>`__.\nThe standard gradient descent is modified as follows:\n\n\\begin{align}\\theta_{t+1} = \\theta_t - \\eta F^{-1}\\nabla \\mathcal{L}(\\theta),\\end{align}\n\nwhere $F$ is the `Fisher information matrix <https://en.wikipedia.org/wiki/Fisher_information#Matrix_form>`__.\nThe Fisher information matrix acts as a metric tensor, transforming the\nsteepest descent in the Euclidean parameter space to the steepest descent in the\ndistribution space.\n\nThe quantum analog\n^^^^^^^^^^^^^^^^^^\n\nIn a similar vein, it has been shown that the standard Euclidean geometry\nis sub-optimal for optimization of quantum variational algorithms\n`(Harrow and Napp, 2019) <https://arxiv.org/abs/1901.05374>`__.\nThe space of quantum states instead possesses a unique invariant metric\ntensor known as the Fubini-Study metric tensor $g_{ij}$, which can be used to\nconstruct a quantum analog to natural gradient descent:\n\n\\begin{align}\\theta_{t+1} = \\theta_t - \\eta g^{+}(\\theta_t)\\nabla \\mathcal{L}(\\theta),\\end{align}\n\nwhere $g^{+}$ refers to the pseudo-inverse.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>It can be shown that the Fubini-Study metric tensor reduces\n    to the Fisher information matrix in the classical limit.\n\n    Furthermore, in the limit where $\\eta\\rightarrow 0$,\n    the dynamics of the system are equivalent to imaginary-time\n    evolution within the variational subspace, as proposed in\n    `McArdle et al. (2018) <https://arxiv.org/abs/1804.03023>`__.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Block-diagonal metric tensor\n----------------------------\n\nA block-diagonal approximation to the Fubini-Study metric tensor\nof a variational quantum circuit can be evaluated on quantum hardware.\n\nConsider a variational quantum circuit\n\n\\begin{align}U(\\mathbf{\\theta})|\\psi_0\\rangle = V_L(\\theta_L) W_L V_{L-1}(\\theta_{L-1}) W_{L-1}\n      \\cdots V_{\\ell}(\\theta_{\\ell}) W_{\\ell} \\cdots V_{0}(\\theta_{0}) W_{0} |\\psi_0\\rangle\\end{align}\n\nwhere\n\n* $|\\psi_0\\rangle$ is the initial state,\n* $W_\\ell$ are layers of non-parametrized quantum gates,\n* $V_\\ell(\\theta_\\ell)$ are layers of parametrized quantum gates\n  with $n_\\ell$ parameters $\\theta_\\ell = \\{\\theta^{(\\ell)}_0, \\dots, \\theta^{(\\ell)}_n\\}$.\n\nFurther, assume all parametrized gates can be written in the form\n$X(\\theta^{(\\ell)}_{i}) = e^{i\\theta^{(\\ell)}_{i} K^{(\\ell)}_i}$,\nwhere $K^{(\\ell)}_i$ is the *generator* of the parametrized operation.\n\nFor each parametric layer $\\ell$ in the variational quantum circuit\nthe $n_\\ell\\times n_\\ell$ block-diagonal submatrix\nof the Fubini-Study tensor $g_{ij}^{(\\ell)}$ is calculated by:\n\n\\begin{align}g_{ij}^{(\\ell)} = \\langle \\psi_{\\ell-1} | K_i K_j | \\psi_{\\ell-1} \\rangle\n    - \\langle \\psi_{\\ell-1} | K_i | \\psi_{\\ell-1}\\rangle\n    \\langle \\psi_{\\ell-1} |K_j | \\psi_{\\ell-1}\\rangle\\end{align}\n\nwhere\n\n\\begin{align}| \\psi_{\\ell-1}\\rangle = V_{\\ell-1}(\\theta_{\\ell-1}) W_{\\ell-1} \\cdots V_{0}(\\theta_{0}) W_{0} |\\psi_0\\rangle.\\end{align}\n\n(that is, $|\\psi_{\\ell-1}\\rangle$ is the quantum state prior to the application\nof parameterized layer $\\ell$), and we have $K_i \\equiv K_i^{(\\ell)}$ for brevity.\n\nLet's consider a small variational quantum circuit example coded in PennyLane:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nimport pennylane as qml\nfrom pennylane import expval, var\n\ndev = qml.device(\"default.qubit\", wires=3)\n\n\n@qml.qnode(dev)\ndef circuit(params):\n    # |psi_0>: state preparation\n    qml.RY(np.pi / 4, wires=0)\n    qml.RY(np.pi / 3, wires=1)\n    qml.RY(np.pi / 7, wires=2)\n\n    # V0(theta0, theta1): Parametrized layer 0\n    qml.RZ(params[0], wires=0)\n    qml.RZ(params[1], wires=1)\n\n    # W1: non-parametrized gates\n    qml.CNOT(wires=[0, 1])\n    qml.CNOT(wires=[1, 2])\n\n    # V_1(theta2, theta3): Parametrized layer 1\n    qml.RY(params[2], wires=1)\n    qml.RX(params[3], wires=2)\n\n    # W2: non-parametrized gates\n    qml.CNOT(wires=[0, 1])\n    qml.CNOT(wires=[1, 2])\n\n    return qml.expval(qml.PauliY(0))\n\n\nparams = np.array([0.432, -0.123, 0.543, 0.233])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above circuit consists of 4 parameters, with two distinct parametrized\nlayers of 2 parameters each.\n\n.. figure:: ../demonstrations/quantum_natural_gradient/qng1.png\n    :align: center\n    :width: 90%\n    :target: javascript:void(0)\n\n|\n\n(Note that in this example, the first non-parametrized layer $W_0$\nis simply the identity.) Since there are two layers, each with two parameters,\nthe block-diagonal approximation consists of two\n$2\\times 2$ matrices, $g^{(0)}$ and $g^{(1)}$.\n\n.. figure:: ../demonstrations/quantum_natural_gradient/qng2.png\n    :align: center\n    :width: 30%\n    :target: javascript:void(0)\n\nTo compute the first block-diagonal $g^{(0)}$, we create subcircuits consisting\nof all gates prior to the layer, and observables corresponding to\nthe *generators* of the gates in the layer:\n\n.. figure:: ../demonstrations/quantum_natural_gradient/qng3.png\n    :align: center\n    :width: 30%\n    :target: javascript:void(0)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g0 = np.zeros([2, 2])\n\n\ndef layer0_subcircuit(params):\n    \"\"\"This function contains all gates that\n    precede parametrized layer 0\"\"\"\n    qml.RY(np.pi / 4, wires=0)\n    qml.RY(np.pi / 3, wires=1)\n    qml.RY(np.pi / 7, wires=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then post-process the measurement results in order to determine $g^{(0)}$,\nas follows.\n\n.. figure:: ../demonstrations/quantum_natural_gradient/qng4.png\n    :align: center\n    :width: 50%\n    :target: javascript:void(0)\n\nWe can see that the diagonal terms are simply given by the variance:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef layer0_diag(params):\n    layer0_subcircuit(params)\n    return var(qml.PauliZ(0)), var(qml.PauliZ(1))\n\n\n# calculate the diagonal terms\nvarK0, varK1 = layer0_diag(params)\ng0[0, 0] = varK0 / 4\ng0[1, 1] = varK1 / 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following two subcircuits are then used to calculate the\noff-diagonal covariance terms of $g^{(0)}$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef layer0_off_diag_single(params):\n    layer0_subcircuit(params)\n    return expval(qml.PauliZ(0)), expval(qml.PauliZ(1))\n\n\n@qml.qnode(dev)\ndef layer0_off_diag_double(params):\n    layer0_subcircuit(params)\n    ZZ = np.kron(np.diag([1, -1]), np.diag([1, -1]))\n    return expval(qml.Hermitian(ZZ, wires=[0, 1]))\n\n\n# calculate the off-diagonal terms\nexK0, exK1 = layer0_off_diag_single(params)\nexK0K1 = layer0_off_diag_double(params)\n\ng0[0, 1] = (exK0K1 - exK0 * exK1) / 4\ng0[1, 0] = (exK0K1 - exK0 * exK1) / 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that, by definition, the block-diagonal matrices must be real and\nsymmetric.\n\nWe can repeat the above process to compute $g^{(1)}$. The subcircuit\nrequired is given by\n\n.. figure:: ../demonstrations/quantum_natural_gradient/qng8.png\n    :align: center\n    :width: 50%\n    :target: javascript:void(0)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g1 = np.zeros([2, 2])\n\n\ndef layer1_subcircuit(params):\n    \"\"\"This function contains all gates that\n    precede parametrized layer 1\"\"\"\n    # |psi_0>: state preparation\n    qml.RY(np.pi / 4, wires=0)\n    qml.RY(np.pi / 3, wires=1)\n    qml.RY(np.pi / 7, wires=2)\n\n    # V0(theta0, theta1): Parametrized layer 0\n    qml.RZ(params[0], wires=0)\n    qml.RZ(params[1], wires=1)\n\n    # W1: non-parametrized gates\n    qml.CNOT(wires=[0, 1])\n    qml.CNOT(wires=[1, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using this subcircuit, we can now generate the submatrix $g^{(1)}$.\n\n.. figure:: ../demonstrations/quantum_natural_gradient/qng5.png\n    :align: center\n    :width: 50%\n    :target: javascript:void(0)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef layer1_diag(params):\n    layer1_subcircuit(params)\n    return var(qml.PauliY(1)), var(qml.PauliX(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As previously, the diagonal terms are simply given by the variance,\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "varK0, varK1 = layer1_diag(params)\ng1[0, 0] = varK0 / 4\ng1[1, 1] = varK1 / 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "while the off-diagonal terms require covariance between the two\nobservables to be computed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef layer1_off_diag_single(params):\n    layer1_subcircuit(params)\n    return expval(qml.PauliY(1)), expval(qml.PauliX(2))\n\n\n@qml.qnode(dev)\ndef layer1_off_diag_double(params):\n    layer1_subcircuit(params)\n    X = np.array([[0, 1], [1, 0]])\n    Y = np.array([[0, -1j], [1j, 0]])\n    YX = np.kron(Y, X)\n    return expval(qml.Hermitian(YX, wires=[1, 2]))\n\n\n# calculate the off-diagonal terms\nexK0, exK1 = layer1_off_diag_single(params)\nexK0K1 = layer1_off_diag_double(params)\n\ng1[0, 1] = (exK0K1 - exK0 * exK1) / 4\ng1[1, 0] = g1[0, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Putting this altogether, the block-diagonal approximation to the Fubini-Study\nmetric tensor for this variational quantum circuit is\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.linalg import block_diag\n\ng = block_diag(g0, g1)\nprint(np.round(g, 8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PennyLane QNodes contain a built-in method for computing the Fubini-Study metric\ntensor, :meth:`~.pennylane.QNode.metric_tensor`, which\nwe can use to verify this result:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(np.round(circuit.metric_tensor([params]), 8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As opposed to our manual computation, which required 6 different quantum\nevaluations, the PennyLane Fubini-Study metric tensor implementation\nrequires only 2 quantum evaluations, one per layer. This is done by\nautomatically detecting the layer structure, and noting that every\nobservable that must be measured commutes, allowing for simultaneous measurement.\n\nTherefore, by combining the quantum natural gradient optimizer with the analytic\nparameter-shift rule to optimize a variational circuit with $d$ parameters\nand $L$ parametrized layers, a total of $2d+L$ quantum evaluations\nare required per optimization step.\n\nNote that the :meth:`~.pennylane.QNode.metric_tensor` method also supports computing the diagonal\napproximation to the metric tensor:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(circuit.metric_tensor([params], diag_approx=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum natural gradient optimization\n-------------------------------------\n\nPennyLane provides an implementation of the quantum natural gradient\noptimizer, :class:`~.pennylane.QNGOptimizer`. Let's compare the optimization convergence\nof the QNG Optimizer and the :class:`~.pennylane.GradientDescentOptimizer` for the simple variational\ncircuit above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "steps = 200\ninit_params = np.array([0.432, -0.123, 0.543, 0.233])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing vanilla gradient descent:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gd_cost = []\nopt = qml.GradientDescentOptimizer(0.01)\n\ntheta = init_params\nfor _ in range(steps):\n    theta = opt.step(circuit, theta)\n    gd_cost.append(circuit(theta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing quantum natural gradient descent:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qng_cost = []\nopt = qml.QNGOptimizer(0.01)\n\ntheta = init_params\nfor _ in range(steps):\n    theta = opt.step(circuit, theta)\n    qng_cost.append(circuit(theta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the cost vs optimization step for both optimization strategies:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n\nplt.style.use(\"seaborn\")\nplt.plot(gd_cost, \"b\", label=\"Vanilla gradient descent\")\nplt.plot(qng_cost, \"g\", label=\"Quantum natural gradient descent\")\n\nplt.ylabel(\"Cost function value\")\nplt.xlabel(\"Optimization steps\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n----------\n\n1. Shun-Ichi Amari. \"Natural gradient works efficiently in learning.\"\n   `Neural computation 10.2,  251-276 <https://www.mitpressjournals.org/doi/abs/10.1162/089976698300017746>`__, 1998.\n\n2. James Stokes, Josh Izaac, Nathan Killoran, Giuseppe Carleo.\n   \"Quantum Natural Gradient.\" `arXiv:1909.02108 <https://arxiv.org/abs/1909.02108>`__, 2019.\n\n3. Aram Harrow and John Napp. \"Low-depth gradient measurements can improve\n   convergence in variational hybrid quantum-classical algorithms.\"\n   `arXiv:1901.05374 <https://arxiv.org/abs/1901.05374>`__, 2019.\n\n4. Naoki Yamamoto. \"On the natural gradient for variational quantum eigensolver.\"\n   `arXiv:1909.05074 <https://arxiv.org/abs/1909.05074>`__, 2019.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}