{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nQuantum transfer learning\n=========================\n\nIn this tutorial we apply a machine learning method, known as *transfer learning*, to an\nimage classifier based on a hybrid classical-quantum network.\n\nThis example follows the general structure of the PyTorch\n`tutorial on transfer learning <https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html>`_\nby Sasank Chilamkurthy, with the crucial difference of using a quantum circuit to perform the\nfinal classification task.\n\nMore details on this topic can be found in the research paper [1] (`Mari et al. (2019) <https://arxiv.org/abs/1912.08278>`_).\n\n\nIntroduction\n------------\n\nTransfer learning is a well-established technique for training artificial neural networks (see e.g., Ref. [2]),\nwhich is based on the general intuition that if a pre-trained network is good at solving a\ngiven problem, then, with just a bit of additional training, it can be used to also solve a different\nbut related problem.\n\nAs discussed in Ref. [1], this idea can be formalized in terms of two abstract netwoks $A$\nand $B$, independently from their quantum or classical physical nature.\n\n|\n\n\n.. figure:: ../demonstrations/quantum_transfer_learning/transfer_learning_general.png\n   :scale: 45%\n   :alt: transfer_general\n   :align: center\n\n|\n\nAs sketched in the above figure, one can give the following **general definition of the\ntransfer learning method**:\n\n1. Take a network $A$ that has been pre-trained on a dataset $D_A$ and for a given\n   task $T_A$.\n\n2. Remove some of the final layers. In this way, the resulting truncated network $A'$\n   can be used as a feature extractor.\n\n3. Connect a new trainable network $B$ at the end of the pre-trained network $A'$.\n\n4. Keep the weights of $A'$ constant, and train the final block $B$ with a\n   new dataset $D_B$ and/or for a new task of interest $T_B$.\n\nWhen dealing with hybrid systems, depending on the physical nature (classical or quantum) of the\nnetworks $A$ and $B$, one can have different demonstrations of transfer learning as\nsummarized in following table:\n\n|\n\n.. rst-class:: docstable\n\n+-----------+-----------+-----------------------------------------------------+\n| Network A | Network B | Tansfer learning scheme                             |\n+===========+===========+=====================================================+\n| Classical | Classical | CC - Standard classical method. See e.g., Ref. [2]. |\n+-----------+-----------+-----------------------------------------------------+\n| Classical | Quantum   | CQ - **Hybrid model presented in this tutorial.**   |\n+-----------+-----------+-----------------------------------------------------+\n| Quantum   | Classical | QC - Model studied in Ref. [1].                     |\n+-----------+-----------+-----------------------------------------------------+\n| Quantum   | Quantum   | QQ - Model studied in Ref. [1].                     |\n+-----------+-----------+-----------------------------------------------------+\n\nClassical-to-quantum transfer learning\n--------------------------------------\n\nWe focus on the CQ transfer learning scheme discussed in the previous section and we give a specific example.\n\n1. As pre-trained network $A$ we use **ResNet18**, a deep residual neural network introduced by\n   Microsoft in Ref. [3], which is pre-trained on the *ImageNet* dataset.\n\n2. After removing its final layer we obtain $A'$, a pre-processing block which maps any\n   input high-resolution image into 512 abstract features.\n\n3. Such features are classified by a 4-qubit \"dressed quantum circuit\" $B$, i.e., a\n   variational quantum circuit sandwiched between two classical layers.\n\n4. The hybrid model is trained, keeping $A'$ constant, on the *Hymenoptera* dataset\n   (a small subclass of ImageNet) containing images of *ants* and *bees*.\n\nA graphical representation of the full data processing pipeline is given in the figure below.\n\n.. figure:: ../demonstrations/quantum_transfer_learning/transfer_learning_c2q.png\n   :scale: 55%\n   :alt: transfer_c2q\n   :align: center\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "General setup\n------------------------\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To use the PyTorch interface in PennyLane, you must first\n   `install PyTorch <https://pytorch.org/get-started/locally/#start-locally>`_.</p></div>\n\nIn addition to *PennyLane*, we will also need some standard *PyTorch* libraries and the\nplotting library *matplotlib*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some parts of this code are based on the Python script:\n# https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py\n# License: BSD\n\n# Plotting\nimport matplotlib.pyplot as plt\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import datasets, models, transforms\n\n# Pennylane\nimport pennylane as qml\nfrom pennylane import numpy as np\n\n# Other tools\nimport time\nimport os\nimport copy\n\n# OpenMP: number of parallel threads.\nos.environ[\"OMP_NUM_THREADS\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting of the main hyper-parameters of the model\n------------------------------------------------------------\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To reproduce the results of Ref. [1], ``num_epochs`` should be set to ``30`` which may take a long time.\n  We suggest to first try with ``num_epochs=1`` and, if everything runs smoothly, increase it to a larger value.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_qubits = 4                # Number of qubits\nstep = 0.0004               # Learning rate\nbatch_size = 4              # Number of samples for each training step\nnum_epochs = 1              # Number of training epochs\nq_depth = 6                 # Depth of the quantum circuit (number of variational layers)\ngamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.\nq_delta = 0.01              # Initial spread of random quantum weights\nrng_seed = 0                # Seed for random number generator\nstart_time = time.time()    # Start of the computation timer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We initialize a PennyLane device with a ``default.qubit`` backend.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=n_qubits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We configure PyTorch to use CUDA only if available. Otherwise the CPU is used.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset loading\n------------------------------------------------------------\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The dataset containing images of *ants* and *bees* can be downloaded\n    `here <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`_ and\n    should be extracted in the subfolder ``../_data/hymenoptera_data``.</p></div>\n\nThis is a very small dataset (roughly 250 images), too small for training from scratch a\nclassical or quantum model, however it is enough when using *transfer learning* approach.\n\nThe PyTorch packages ``torchvision`` and ``torch.utils.data`` are used for loading the dataset\nand performing standard preliminary image operations: resize, center, crop, normalize, *etc.*\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n    \"train\": transforms.Compose(\n        [\n            # transforms.RandomResizedCrop(224),     # uncomment for data augmentation\n            # transforms.RandomHorizontalFlip(),     # uncomment for data augmentation\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            # Normalize input channels using mean values and standard deviations of ImageNet.\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    ),\n    \"val\": transforms.Compose(\n        [\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    ),\n}\n\ndata_dir = \"../_data/hymenoptera_data\"\nimage_datasets = {\n    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [\"train\", \"val\"]\n}\ndataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\"]}\nclass_names = image_datasets[\"train\"].classes\n\n# Initialize dataloader\ndataloaders = {\n    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True)\n    for x in [\"train\", \"val\"]\n}\n\n# function to plot images\ndef imshow(inp, title=None):\n    \"\"\"Display image from tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    # Inverse of the initial normalization operation.\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us show a batch of the test data, just to have an idea of the classification problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get a batch of training data\ninputs, classes = next(iter(dataloaders[\"val\"]))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n\n# In order to get reproducible results, we set a manual seed for the\n# random number generator and re-initialize the dataloaders.\n\ntorch.manual_seed(rng_seed)\ndataloaders = {\n    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True)\n    for x in [\"train\", \"val\"]\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variational quantum circuit\n------------------------------------\nWe first define some quantum layers that will compose the quantum circuit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def H_layer(nqubits):\n    \"\"\"Layer of single-qubit Hadamard gates.\n    \"\"\"\n    for idx in range(nqubits):\n        qml.Hadamard(wires=idx)\n\n\ndef RY_layer(w):\n    \"\"\"Layer of parametrized qubit rotations around the y axis.\n    \"\"\"\n    for idx, element in enumerate(w):\n        qml.RY(element, wires=idx)\n\n\ndef entangling_layer(nqubits):\n    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n    \"\"\"\n    # In other words it should apply something like :\n    # CNOT  CNOT  CNOT  CNOT...  CNOT\n    #   CNOT  CNOT  CNOT...  CNOT\n    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n        qml.CNOT(wires=[i, i + 1])\n    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n        qml.CNOT(wires=[i, i + 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define the quantum circuit through the PennyLane `qnode` decorator .\n\nThe structure is that of a typical variational quantum circuit:\n\n* **Embedding layer:** All qubits are first initialized in a balanced superposition\n  of *up* and *down* states, then they are rotated according to the input parameters\n  (local embedding).\n\n* **Variational layers:** A sequence of trainable rotation layers and constant\n  entangling layers is applied.\n\n* **Measurement layer:** For each qubit, the local expectation value of the $Z$\n  operator is measured. This produces a classical output vector, suitable for\n  additional post-processing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\ndef q_net(q_in, q_weights_flat):\n\n    # Reshape weights\n    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n\n    # Start from state |+> , unbiased w.r.t. |0> and |1>\n    H_layer(n_qubits)\n\n    # Embed features in the quantum node\n    RY_layer(q_in)\n\n    # Sequence of trainable variational layers\n    for k in range(q_depth):\n        entangling_layer(n_qubits)\n        RY_layer(q_weights[k])\n\n    # Expectation values in the Z basis\n    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n    return tuple(exp_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dressed quantum circuit\n------------------------\n\nWe can now define a custom ``torch.nn.Module`` representing a *dressed* quantum circuit.\n\nThis is a concatenation of:\n\n* A classical pre-processing layer (``nn.Linear``).\n* A classical activation function (``torch.tanh``).\n* A constant ``np.pi/2.0`` scaling.\n* The previously defined quantum circuit (``q_net``).\n* A classical post-processing layer (``nn.Linear``).\n\nThe input of the module is a batch of vectors with 512 real parameters (features) and\nthe output is a batch of vectors with two real outputs (associated with the two classes\nof images: *ants* and *bees*).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Quantumnet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pre_net = nn.Linear(512, n_qubits)\n        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n        self.post_net = nn.Linear(n_qubits, 2)\n\n    def forward(self, input_features):\n        pre_out = self.pre_net(input_features)\n        q_in = torch.tanh(pre_out) * np.pi / 2.0\n\n        # Apply the quantum circuit to each element of the batch and append to q_out\n        q_out = torch.Tensor(0, n_qubits)\n        q_out = q_out.to(device)\n        for elem in q_in:\n            q_out_elem = q_net(elem, self.q_params).float().unsqueeze(0)\n            q_out = torch.cat((q_out, q_out_elem))\n        return self.post_net(q_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hybrid classical-quantum model\n------------------------------------\n\nWe are finally ready to build our full hybrid classical-quantum network.\nWe follow the *transfer learning* approach:\n\n1. First load the classical pre-trained network *ResNet18* from the ``torchvision.models`` zoo.\n2. Freeze all the weights since they should not be trained.\n3. Replace the last fully connected layer with our trainable dressed quantum circuit (``Quantumnet``).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The *ResNet18* model is automatically downloaded by PyTorch and it may take several minutes (only the first time).</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_hybrid = torchvision.models.resnet18(pretrained=True)\n\nfor param in model_hybrid.parameters():\n    param.requires_grad = False\n\n\n# Notice that model_hybrid.fc is the last layer of ResNet18\nmodel_hybrid.fc = Quantumnet()\n\n# Use CUDA or CPU according to the \"device\" object.\nmodel_hybrid = model_hybrid.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training and results\n------------------------\n\nBefore training the network we need to specify the *loss* function.\n\nWe use, as usual in classification problem, the *cross-entropy* which is\ndirectly available within ``torch.nn``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also initialize the *Adam optimizer* which is called at each training step\nin order to update the weights of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer_hybrid = optim.Adam(model_hybrid.fc.parameters(), lr=step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We schedule to reduce the learning rate by a factor of ``gamma_lr_scheduler``\nevery 10 epochs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_hybrid, step_size=10, gamma=gamma_lr_scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What follows is a training function that will be called later.\nThis function should return a trained model that can be used to make predictions\n(classifications).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    best_loss = 10000.0  # Large arbitrary number\n    best_acc_train = 0.0\n    best_loss_train = 10000.0  # Large arbitrary number\n    print(\"Training started:\")\n\n    for epoch in range(num_epochs):\n\n        # Each epoch has a training and validation phase\n        for phase in [\"train\", \"val\"]:\n            if phase == \"train\":\n                scheduler.step()\n                # Set model to training mode\n                model.train()\n            else:\n                # Set model to evaluate mode\n                model.eval()\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            n_batches = dataset_sizes[phase] // batch_size\n            it = 0\n            for inputs, labels in dataloaders[phase]:\n                since_batch = time.time()\n                batch_size_ = len(inputs)\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                optimizer.zero_grad()\n\n                # Track/compute gradient and make an optimization step only when training\n                with torch.set_grad_enabled(phase == \"train\"):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n\n                # Print iteration results\n                running_loss += loss.item() * batch_size_\n                batch_corrects = torch.sum(preds == labels.data).item()\n                running_corrects += batch_corrects\n                print(\n                    \"Phase: {} Epoch: {}/{} Iter: {}/{} Batch time: {:.4f}\".format(\n                        phase,\n                        epoch + 1,\n                        num_epochs,\n                        it + 1,\n                        n_batches + 1,\n                        time.time() - since_batch,\n                    ),\n                    end=\"\\r\",\n                    flush=True,\n                )\n                it += 1\n\n            # Print epoch results\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n            print(\n                \"Phase: {} Epoch: {}/{} Loss: {:.4f} Acc: {:.4f}        \".format(\n                    \"train\" if phase == \"train\" else \"val  \",\n                    epoch + 1,\n                    num_epochs,\n                    epoch_loss,\n                    epoch_acc,\n                )\n            )\n\n            # Check if this is the best model wrt previous epochs\n            if phase == \"val\" and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == \"val\" and epoch_loss < best_loss:\n                best_loss = epoch_loss\n            if phase == \"train\" and epoch_acc > best_acc_train:\n                best_acc_train = epoch_acc\n            if phase == \"train\" and epoch_loss < best_loss_train:\n                best_loss_train = epoch_loss\n\n    # Print final results\n    model.load_state_dict(best_model_wts)\n    time_elapsed = time.time() - since\n    print(\"Training completed in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60))\n    print(\"Best test loss: {:.4f} | Best test accuracy: {:.4f}\".format(best_loss, best_acc))\n    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are ready to perform the actual training process.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_hybrid = train_model(\n    model_hybrid, criterion, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing the model predictions\n------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first define a visualization function for a batch of test data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, num_images=6, fig_name=\"Predictions\"):\n    images_so_far = 0\n    _fig = plt.figure(fig_name)\n    model.eval()\n    with torch.no_grad():\n        for _i, (inputs, labels) in enumerate(dataloaders[\"val\"]):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images // 2, 2, images_so_far)\n                ax.axis(\"off\")\n                ax.set_title(\"[{}]\".format(class_names[preds[j]]))\n                imshow(inputs.cpu().data[j])\n                if images_so_far == num_images:\n                    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can run the previous function to see a batch of images\nwith the corresponding predictions.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "visualize_model(model_hybrid, num_images=batch_size)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n------------\n\n[1] Andrea Mari, Thomas R. Bromley, Josh Izaac, Maria Schuld, and Nathan Killoran.\n*Transfer learning in hybrid classical-quantum neural networks*. arXiv:1912.08278 (2019).\n\n[2] Rajat Raina, Alexis Battle, Honglak  Lee,  Benjamin Packer, and Andrew Y Ng.\n*Self-taught learning:  transfer learning from unlabeled data*.\nProceedings of the 24th International  Conference  on  Machine  Learning*, 759\u2013766 (2007).\n\n[3] Kaiming He, Xiangyu Zhang, Shaoqing ren and Jian Sun. *Deep residual learning for image recognition*.\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770-778 (2016).\n\n[4] Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, Carsten Blank, Keri McKiernan, and Nathan Killoran.\n*PennyLane: Automatic differentiation of hybrid quantum-classical computations*. arXiv:1811.04968 (2018).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}