{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nQuantum embeddings and metric learning\n======================================\n\n**Authors: Maria Schuld and Aroosa Ijaz**\n\nThis tutorial illustrates the idea of quantum embeddings for metric\nlearning presented in `Lloyd, Schuld, Ijaz, Izaac, Killoran (2019) <https://arxiv.org/abs/2001.03622>`_,\nby training a hybrid classical-quantum data\nembedding to classify images of ants and bees. The example was inspired\nby `Mari et al. (2019) <https://arxiv.org/abs/1912.08278>`_,\n(see also this `tutorial <https://pennylane.ai/qml/app/tutorial_quantum_transfer_learning.html>`_),\nand reproduces some of the subplots in Figure 5 of Lloyd et al.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tutorial requires the following imports:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nimport pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane import RX, RY, RZ, CNOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Idea\n----\n\nQuantum metric learning trains a quantum embedding\u2014for example, a\nquantum circuit that encodes classical data into quantum states\u2014to\nseparate different classes of data in the Hilbert space of the quantum\nsystem.\n\n.. figure:: ../implementations/embedding_metric_learning/training.png\n   :align: center\n   :width: 40%\n\nThe trained embedding can be used for classification. A new data sample\n(red dot) gets mapped into Hilbert space via the same embedding, and a\nspecial measurement compares it to the two embedded classes.\nThe decision boundary of the measurement in quantum state space is nearly\nlinear (red dashed line).\n\n.. figure:: ../implementations/embedding_metric_learning/classification.png\n   :align: center\n   :width: 40%\n\nSince a simple metric in Hilbert space corresponds to a potentially much\nmore complex metric in the original data space, the simple decision\nboundary can translate to a non-trivial decision boundary in the\noriginal space of the data.\n\n.. figure:: ../implementations/embedding_metric_learning/dec_boundary.png\n   :align: center\n   :width: 40%\n\nThe best quantum measurement one could construct to classify new inputs\ndepends on the loss defined for the classification task, as well as the\nmetric used to optimize the separation of data.\n\nFor a linear cost function, data separated by the trace distance or\n$\\ell_1$ metric is best distinguished by a Helstrom measurement, while\ndata separated by the Hilbert-Schmidt distance or $\\ell_2$ metric\nis best classified by a fidelity measurement. Here we show how to\nimplement training and classification based on the $\\ell_2$\nmetric.\n\nEmbedding\n---------\n\nA quantum embedding is a representation of data points $x$ from a\ndata domain $X$ as a *(quantum) feature state*\n$| x \\rangle$. Either the full embedding, or part of it, can be\nfacilitated by a \"quantum feature map\", a quantum circuit\n$\\Phi(x)$ that depends on the input. If the circuit has additional\nparameters $\\theta$ that are adaptable,\n$\\Phi = \\Phi(x, \\theta)$, the quantum feature map can be trained\nvia optimization.\n\nIn this tutorial we investigate a trainable, hybrid classical-quantum embedding\nimplemented by a partially pre-trained classical neural network,\nfollowed by a parametrized quantum circuit that implements the quantum\nfeature map:\n\n|\n\n.. figure:: ../implementations/embedding_metric_learning/pipeline.png\n   :align: center\n   :width: 100%\n\n|\n\nFollowing `Mari et al. (2019) <https://arxiv.org/abs/1912.08278>`__,\nfor the classical neural network we use PyTorch's\n``torch.models.resnet18()``, setting ``pretrained=True``. The final\nlayer of the ResNet, which usually maps a 512-dimensional vector to 1000\nnodes representing different image classes, is replaced by a linear\nlayer of 2 output neurons. The classical part of the embedding therefore\nmaps the images to a 2-dimensional *intermediate feature space*.\n\nFor the quantum part we use the QAOA embedding proposed\nin `Lloyd et al. (2019) <https://arxiv.org/abs/2001.03622>`_.\nThe feature map is represented by a layered variational circuit, which\nalternates a \"feature-encoding Hamiltonian\" and an \"Ising-like\" Hamiltonian\nwith ZZ-entanglers (the two-qubit gates in the circuit diagram above) and ``RY`` gates as local fields.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def feature_encoding_hamiltonian(features, wires):\n\n    for idx, w in enumerate(wires):\n        RX(features[idx], wires=w)\n\ndef ising_hamiltonian(weights, wires, l):\n\n        # ZZ coupling\n        CNOT(wires=wires)\n        RZ(2 * weights[l, 0], wires=wires[0])\n        CNOT(wires=wires)\n        # local fields\n        for w in wires:\n            RY(weights[l, i + 1], wires=w)\n\ndef QAOAEmbedding(features, weights, wires):\n\n    repeat = len(weights)\n    for l in range(repeat):\n        # apply alternating Hamiltonians\n        feature_encoding_hamiltonian(features, wires)\n        ising_hamiltonian(weights, wires, l)\n    # repeat the feature encoding once more at the end\n    feature_encoding_hamiltonian(features, wires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overall, the embedding has 1024 + 12 trainable parameters - 1024 for the\nclassical part of the model and 12 for the four layers of the QAOA\nembedding.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The pretrained neural network has already learned\n          to separate the data. The example does therefore not\n          make any claims on the performance of the embedding, but aims to\n          illustrate how a hybrid embedding can be trained.</p></div>\n\nData\n----\n\nWe consider a binary supervised learning problem with examples\n$\\{a_1,...a_{M_a}\\} \\subseteq X$ from class $A$ and examples\n$\\{b_1,...b_{M_b}\\} \\subseteq X$ from class $B$. The data\nare images of ants ($A$) and bees ($B$), taken from `Kaggle's\nhymenoptera dataset <https://www.kaggle.com/ajayrana/hymenoptera-data>`__.\nThis is a sample of four images:\n\n.. figure:: ../implementations/embedding_metric_learning/data_example.png\n   :align: center\n   :width: 50%\n\nFor convenience, instead of coding up the classical neural network, we\nload `pre-extracted feature vectors of the images\n<https://github.com/XanaduAI/qml/blob/master/implementations/embedding_metric_learning/X_antbees.txt>`_.\nThese were created by\nresizing, cropping and normalizing the images, and passing them through\nPyTorch's pretrained ResNet 512 (that is, without the final linear\nlayer).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = np.loadtxt(\"embedding_metric_learning/X_antbees.txt\", ndmin=2)  #1  pre-extracted inputs\nY = np.loadtxt(\"embedding_metric_learning/Y_antbees.txt\")  # labels\nX_val = np.loadtxt(\n    \"embedding_metric_learning/X_antbees_test.txt\", ndmin=2\n)  # pre-extracted validation inputs\nY_val = np.loadtxt(\"embedding_metric_learning/Y_antbees_test.txt\")  # validation labels\n\n# split data into two classes\nA = X[Y == -1]\nB = X[Y == 1]\nA_val = X_val[Y_val == -1]\nB_val = X_val[Y_val == 1]\n\nprint(A.shape)\nprint(B.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cost\n----\n\nThe distance metric underlying the notion of 'separation' is the\n$\\ell_2$ or Hilbert-Schmidt norm, which depends on overlaps of\nthe embedded data points $|a\\rangle$\nfrom class $A$ and $|b\\rangle$ from class $B$,\n\n\\begin{align}D_{\\mathrm{hs}}(A, B) =  \\frac{1}{2} \\big( \\sum_{i, i'} |\\langle a_i|a_{i'}\\rangle|^2\n       +  \\sum_{j,j'} |\\langle b_j|b_{j'}\\rangle|^2 \\big)\n       - \\sum_{i,j} |\\langle a_i|b_j\\rangle|^2.\\end{align}\n\nTo maximize the $\\ell_2$ distance between the two classes in\nHilbert space, we minimize the cost\n$C = 1 - \\frac{1}{2}D_{\\mathrm{hs}}(A, B)$.\n\nTo set up the \"quantum part\" of the cost function in PennyLane, we have\nto create a quantum node. Here, the quantum node is simulated on\nPennyLane's ``'default.qubit'`` backend.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>One could also connect the\n          quantum node to a hardware backend to find out if the noise of a\n          physical implementation still allows us to train the embedding.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_features = 2\nn_qubits = 2 * n_features + 1\n\ndev = qml.device(\"default.qubit\", wires=n_qubits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use a SWAP test to measure the overlap\n$|\\langle \\psi | \\phi \\rangle|^2$ between two quantum feature\nstates $|\\psi\\rangle$ and $|\\phi\\rangle$, prepared by a\n``QAOAEmbedding`` with weights ``q_weights``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef swap_test(q_weights, x1, x2):\n\n    # load the two inputs into two different registers\n    QAOAEmbedding(features=x1, weights=q_weights, wires=[1, 2])\n    QAOAEmbedding(features=x2, weights=q_weights, wires=[3, 4])\n\n    # perform the SWAP test\n    qml.Hadamard(wires=0)\n    for k in range(n_features):\n        qml.CSWAP(wires=[0, k + 1, 2 + k + 1])\n    qml.Hadamard(wires=0)\n\n    return qml.expval(qml.PauliZ(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before executing the swap test, the feature vectors have to be\nmultiplied by a (2, 512)-dimensional matrix that represents the weights\nof the linear layer. This trainable classical pre-processing is executed\nbefore calling the swap test:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def overlaps(weights, X1=None, X2=None):\n\n    linear_layer = weights[0]\n    q_weights = weights[1]\n\n    overlap = 0\n    for x1 in X1:\n        for x2 in X2:\n            # multiply the inputs with the linear layer weight matrix\n            w_x1 = linear_layer @ x1\n            w_x2 = linear_layer @ x2\n            # overlap of embedded intermediate features\n            overlap += swap_test(q_weights, w_x1, w_x2)\n\n    mean_overlap = overlap / (len(X1) * len(X2))\n    return mean_overlap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the ``overlaps()`` function, ``weights`` is a list of two arrays, the first\nrepresenting the matrix of the linear layer, and the second containing\nthe quantum circuit parameters.\n\nWith this we can define the cost function $C$, which depends on\ninter- and intra-cluster overlaps.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(weights, A=None, B=None):\n\n    aa = overlaps(weights, X1=A, X2=A)\n    bb = overlaps(weights, X1=B, X2=B)\n    ab = overlaps(weights, X1=A, X2=B)\n\n    d_hs = -2 * ab + (aa + bb)\n\n    return 1 - 0.5 * d_hs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization\n------------\nThe initial parameters for the trainable classical and quantum part of the embedding are\nchosen at random. The number of layers in the quantum circuit is derived from the first\ndimension of `init_pars_quantum`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# generate initial parameters for circuit\ninit_pars_quantum = np.random.normal(loc=0, scale=0.1, size=(4, 3))\n\n# generate initial parameters for linear layer\ninit_pars_classical = np.random.normal(loc=0, scale=0.1, size=(2, 512))\n\ninit_pars = [init_pars_classical, init_pars_quantum]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now train the embedding with an ``RMSPropOptimizer``, sampling\nfive training points from each class in every step, here shown for 2 steps.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = qml.RMSPropOptimizer(stepsize=0.01)\nbatch_size = 5\npars = init_pars\n\nfor i in range(2):\n\n    # Sample a batch of training inputs from each class\n    selectA = np.random.choice(range(len(A)), size=(batch_size,), replace=True)\n    selectB = np.random.choice(range(len(B)), size=(batch_size,), replace=True)\n    A_batch = [A[s] for s in selectA]\n    B_batch = [B[s] for s in selectB]\n\n    # Walk one optimization step\n    pars = optimizer.step(lambda w: cost(w, A=A_batch, B=B_batch), pars)\n    print(\"Step\", i, \"done.\")\n\n    # Print the validation cost every 10 steps\n    if i % 5 == 0 and i != 0:\n        cst = cost(pars, A=A_val, B=B_val)\n        print(\"Cost on validation set {:2f}\".format(cst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimizing a hybrid quantum-classical model with 1024 + 12 parameters\ntakes an awfully long time. We will\ntherefore load a set of `already trained parameters\n<https://github.com/XanaduAI/qml/blob/master/implementations/embedding_metric_learning/pretrained_parameters.npy>`_\n(from running the cell above for 1500 steps).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Training is sensitive to the hyperparameters</p></div>\nsuch as the batch size, initial parameters and\noptimizer used.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pretrained_pars = np.load(\"embedding_metric_learning/pretrained_parameters.npy\",\n                          allow_pickle=True)\n\nprint(pretrained_pars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analysis\n--------\n\nLet us analyze the effect of training. To speed up the script, we will\nonly look at a reduced version of the training and validation set,\nselecting the first 10 points from either class.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "select = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First of all, the final cost with the pre-trained parameters is as\nfollows:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cost_train = cost(pretrained_pars, A=A[:select], B=B[:select])\ncost_val = cost(pretrained_pars, A=A_val[:select], B=B_val[:select])\nprint(\"Cost for pretrained parameters on training set:\", cost_train)\nprint(\"Cost for pretrained parameters on validation set:\", cost_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A useful way to visualize the distance of data points is to plot a Gram\nmatrix of the overlaps of different feature states. For this we join the\nfirst 10 examples of each of the two classes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "A_B = np.r_[A[:select], B[:select]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before training, the separation between the classes is not recognizable\nin the Gram matrix:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gram_before = [[overlaps(init_pars, X1=[x1], X2=[x2]) for x1 in A_B] for x2 in A_B]\n\nax = plt.subplot(111)\nim = ax.matshow(gram_before, vmin=0, vmax=1)\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nplt.colorbar(im, cax=cax)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training, the gram matrix clearly separates the two classes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gram_after = [[overlaps(pretrained_pars, X1=[x1], X2=[x2]) for x1 in A_B] for x2 in A_B]\n\nax = plt.subplot(111)\nim = ax.matshow(gram_after, vmin=0, vmax=1)\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\nplt.colorbar(im, cax=cax)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also visualize the \"intermediate layer\" of 2-dimensional vectors\n$(x_1, x_2)$, just before feeding them into the quantum circuit.\nBefore training the (2, 512)-dimensional weight matrix of the linear\nlayer, the classes are arbitrarily intermixed.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for a in A:\n    intermediate_a = init_pars[0] @ a\n    plt.scatter(intermediate_a[:][0], intermediate_a[:][1], c=\"red\")\n\nfor b in B:\n    intermediate_b = init_pars[0] @ b\n    plt.scatter(intermediate_b[:][0], intermediate_b[:][1], c=\"blue\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, after training, the linear layer learned to arrange the\nintermediate feature vectors on a periodic grid.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for a in A:\n    intermediate_a = pretrained_pars[0] @ a\n    plt.scatter(intermediate_a[:][0], intermediate_a[:][1], c=\"red\")\n\nfor b in B:\n    intermediate_b = pretrained_pars[0] @ b\n    plt.scatter(intermediate_b[:][0], intermediate_b[:][1], c=\"blue\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification\n--------------\n\nGiven a new input $x \\in X$, and its quantum feature state\n$|x \\rangle$, the trained embedding can be used to solve the\nbinary classification problem of assigning $x$ to either $A$\nor $B$. For an embedding separating data via the $\\ell_2$\nmetric, a very simple measurement can be used for classification: one\ncomputes the overlap of $|x \\rangle$ with examples of\n$|a \\rangle$ and $|b \\rangle$. $x$ is assigned to the\nclass with which it has a larger average overlap in the space of the\nembedding.\n\nLet us consider a picture of an ant from the validation set (assuming\nour model never saw it during training):\n\n|\n\n.. figure:: ../implementations/embedding_metric_learning/ant.jpg\n   :align: center\n   :width: 40%\n\n|\n\nAfter passing it through the classical neural network (excluding the final\nlinear layer), the 512-dimensional feature vector is given by\n``A_val[0]``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_new = A_val[0]\n\nprint(x_new.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We compare the new input with randomly selected samples. The more\nsamples used, the smaller the variance in the prediction.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 200\n\nprediction = 0\nfor s in range(n_samples):\n\n    # select a random sample from the training set\n    sample_index = np.random.choice(len(X))\n    x = X[sample_index]\n    y = Y[sample_index]\n\n    # compute the overlap between training sample and new input\n    overlap = overlaps(pretrained_pars, X1=[x], X2=[x_new])\n\n    # add the label weighed by the overlap to the prediction\n    prediction += y * overlap\n\n# normalize prediction\nprediction = prediction / n_samples\nprint(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the result is negative, the new data point is (correctly) predicted\nto be a picture of an ant, which was the class with -1 labels.\n\nReferences\n----------\nSeth Lloyd, Maria Schuld, Aroosa Ijaz, Josh Izaac, Nathan Killoran: \"Quantum embeddings for machine learning\"\narXiv preprint arXiv:2001.03622.\n\nAndrea Mari, Thomas R. Bromley, Josh Izaac, Maria Schuld, Nathan Killoran: \"Transfer learning\nin hybrid classical-quantum neural networks\" arXiv preprint arXiv:1912.08278\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}