{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n3-qubit Ising model in PyTorch\n==============================\n\nThe interacting spins with variable coupling strengths of an `Ising model <https://en.wikipedia.org/wiki/Ising_model>`__ can be used to simulate various machine learning concepts like `Hopfield networks <https://en.wikipedia.org/wiki/Hopfield_network>`__ and `Boltzmann machines <https://en.wikipedia.org/wiki/Boltzmann_machine>`__ :cite:`schuld2018supervised`. They also closely imitate the underlying mathematics of a subclass of computational problems called\n`Quadratic Unconstrained Binary Optimization (QUBO) <https://en.wikipedia.org/wiki/Quadratic_unconstrained_binary_optimization>`__ problems. \n\nIsing models are commonly encountered in the subject area of adiabatic quantum computing. Quantum annealing algorithms (for example, as performed on a D-wave system) are often used to find low-energy configurations of Ising problems.\nThe optimization landscape of the Ising model is non-convex, which can make finding global minima challenging. In this tutorial, we get a closer look at this phenomenon by applying gradient descent techniques to a toy Ising model.\u00a0 \n\nPennyLane implementation\n------------------------\n\nThis basic tutorial optimizes a 3-qubit Ising model using the PennyLane ``default.qubit``\ndevice with PyTorch. In the absence of external fields, the Hamiltonian for this system is given by:\n\n\\begin{align}H=-\\sum_{<i,j>} J_{ij} \\sigma_i \\sigma_{j},\\end{align}\n\nwhere each spin can be in the +1 or -1 spin state and $J_{ij}$ are the nearest-neighbour coupling strengths.\n\nFor simplicity, the first spin can be assumed\nto be in the \"up\" state (+1 eigenstate of Pauli-Z operator) and the coupling matrix can be set to $J = [1,-1]$. The rotation angles for the other two spins are then optimized\nso that the energy of the system is minimized for the given couplings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.autograd import Variable\nimport pennylane as qml\nfrom pennylane import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A three-qubit quantum circuit is initialized to represent the three spins:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=3)\n\n@qml.qnode(dev, interface=\"torch\") \ndef circuit(p1, p2):\n    # We use the general Rot(phi,theta,omega,wires) single-qubit operation\n    qml.Rot(p1[0], p1[1], p1[2], wires=1)\n    qml.Rot(p2[0], p2[1], p2[2], wires=2)\n    return [qml.expval(qml.PauliZ(i)) for i in range(3)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cost function to be minimized is defined as the energy of the spin configuration:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(var1, var2):\n    # the circuit function returns a numpy array of Pauli-Z expectation values\n    spins = circuit(var1, var2)\n\n    # the expectation value of Pauli-Z is +1 for spin up and -1 for spin down\n    energy = -(1 * spins[0] * spins[1]) - (-1 * spins[1] * spins[2])\n    return energy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sanity check\n^^^^^^^^^^^^^\nLet's test the functions above using the $[s_1, s_2, s_3] = [1, -1, -1]$ spin\nconfiguration and the given coupling matrix. The total energy for this Ising model\nshould be:\n\n\\begin{align}H = -1(J_1 s_1 \\otimes s_2 + J_2 s_2 \\otimes s3) = 2\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test1 = torch.tensor([0, np.pi, 0])\ntest2 = torch.tensor([0, np.pi, 0])\n\ncost_check = cost(test1, test2)\nprint(\"Energy for [1, -1, -1] spin configuration:\", cost_check)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random initialization\n^^^^^^^^^^^^^^^^^^^^^\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(56)\np1 = Variable((np.pi * torch.rand(3, dtype=torch.float64)), requires_grad=True)\np2 = Variable((np.pi * torch.rand(3, dtype=torch.float64)), requires_grad=True)\n\nvar_init = [p1, p2]\ncost_init = cost(p1, p2)\n\nprint(\"Randomly initialized angles:\", var_init)\nprint(\"Corresponding cost before optimization:\", cost_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization\n^^^^^^^^^^^^\nNow we use the PyTorch gradient descent optimizer to minimize the cost:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.SGD(var_init, lr=0.1)\n\ndef closure():\n    opt.zero_grad()\n    loss = cost(p1, p2)\n    loss.backward()\n    return loss\n\nvar_pt = [var_init]\ncost_pt = [cost_init]\nx = [0]\n\nfor i in range(100):\n    opt.step(closure)\n    if (i + 1) % 5 == 0:\n        x.append(i)\n        p1n, p2n = opt.param_groups[0][\"params\"]\n        costn = cost(p1n, p2n)\n        var_pt.append([p1n, p2n])\n        cost_pt.append(costn)\n\n        # for clarity, the angles are printed as numpy arrays\n        print(\"Energy after step {:5d}: {: .7f} | Angles: {}\".format(i+1, costn, [p1n.detach().numpy(), p2n.detach().numpy()]),\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>When using the *PyTorch* optimizer, keep in mind that:\n\n    1. ``loss.backward()`` computes the gradient of the cost function with respect to all parameters with ``requires_grad=True``. \n    2. ``opt.step()`` performs the parameter update based on this *current* gradient and the learning rate. \n    3. ``opt.zero_grad()`` sets all the gradients back to zero. It's important to call this before ``loss.backward()`` to avoid the accumulation of gradients from multiple passes.\n\n    Hence, its standard practice to define the ``closure()`` function that clears up the old gradient, \n    evaluates the new gradient and passes it onto the optimizer in each step.</p></div>\n\nThe minimum energy is -2 for the spin configuration $[s_1, s_2, s_3] = [1, 1, -1]$\nwhich corresponds to\n$(\\phi, \\theta, \\omega) = (0, 0, 0)$ for the second spin and $(\\phi, \\theta, \\omega) = (0, \\pi, 0)$ for \nthe third spin. Note that gradient descent optimization might not find this global minimum due to the non-convex cost function, as is shown in the next section.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p1_final, p2_final = opt.param_groups[0][\"params\"]\nprint(\"Optimized angles:\", p1_final, p2_final)\nprint(\"Final cost after optimization:\", cost(p1_final, p2_final))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(6, 4))\nplt.plot(x, cost_pt, label = 'global minimum')\nplt.xlabel(\"Optimization steps\")\nplt.ylabel(\"Cost / Energy\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Local minimum\n^^^^^^^^^^^^^\nIf the spins are initialized close to the local minimum of zero energy, the optimizer is\nlikely to get stuck here and never find the global minimum at -2. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(9)\np3 = Variable((np.pi*torch.rand(3, dtype = torch.float64)), requires_grad = True)\np4 = Variable((np.pi*torch.rand(3, dtype = torch.float64)), requires_grad = True)\n\nvar_init_loc = [p3, p4]\ncost_init_loc = cost(p3, p4)\n\nprint(\"Corresponding cost before optimization:\", cost_init_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.SGD(var_init_loc, lr = 0.1)\n\ndef closure():\n    opt.zero_grad()\n    loss = cost(p3, p4)\n    loss.backward()\n    return loss\n\nvar_pt_loc = [var_init_loc]\ncost_pt_loc = [cost_init_loc]\n\nfor j in range(100):\n    opt.step(closure)\n    if (j + 1) % 5 == 0:\n        p3n, p4n = opt.param_groups[0]['params']\n        costn = cost(p3n, p4n)\n        var_pt_loc.append([p3n, p4n])\n        cost_pt_loc.append(costn)\n\n        # for clarity, the angles are printed as numpy arrays\n        print('Energy after step {:5d}: {: .7f} | Angles: {}'.format(j+1, costn, [p3n.detach().numpy(), p4n.detach().numpy()]),\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 4))\nplt.plot(x, cost_pt_loc, 'r', label = 'local minimum')\nplt.xlabel(\"Optimization steps\")\nplt.ylabel(\"Cost / Energy\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|\n\nTry it yourself! Download and run this file with different\ninitialization parameters and see how the results change.\n\nFurther reading\n^^^^^^^^^^^^^^^\n\n1. Maria Schuld and Francesco Petruccione. \"Supervised Learning with Quantum Computers.\"\nSpringer, 2018.\n\n2. Andrew Lucas. \"Ising formulations of many NP problems.\"\n`arXiv:1302.5843 <https://arxiv.org/pdf/1302.5843>`__, 2014.\n\n3. Gary Kochenberger et al. \"The Unconstrained Binary Quadratic Programming Problem: A Survey.\"\n`Journal of Combinatorial Optimization <https://link.springer.com/article/10.1007/s10878-014-9734-0>`__, 2014.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}